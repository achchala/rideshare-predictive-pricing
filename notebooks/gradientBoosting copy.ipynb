{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>name</th>\n",
       "      <th>distance</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th>precipIntensity</th>\n",
       "      <th>humidity</th>\n",
       "      <th>temperatureHigh</th>\n",
       "      <th>apparentTemperatureHigh</th>\n",
       "      <th>uvIndex</th>\n",
       "      <th>precipIntensityMax</th>\n",
       "      <th>temperatureMax</th>\n",
       "      <th>apparentTemperatureMax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>42.52</td>\n",
       "      <td>40.53</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>42.52</td>\n",
       "      <td>40.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.66</td>\n",
       "      <td>33.83</td>\n",
       "      <td>32.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>33.83</td>\n",
       "      <td>32.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>33.83</td>\n",
       "      <td>32.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>33.83</td>\n",
       "      <td>32.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0567</td>\n",
       "      <td>0.86</td>\n",
       "      <td>43.83</td>\n",
       "      <td>38.38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1252</td>\n",
       "      <td>43.83</td>\n",
       "      <td>38.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.64</td>\n",
       "      <td>33.83</td>\n",
       "      <td>32.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>33.83</td>\n",
       "      <td>32.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  name  distance  source  destination  precipIntensity  humidity  \\\n",
       "0   12.0     4      1.11       6           11           0.0000      0.60   \n",
       "1   16.0     0      1.11       6           11           0.0000      0.66   \n",
       "2    7.5     3      1.11       6           11           0.0000      0.56   \n",
       "3    7.5     5      1.11       6           11           0.0567      0.86   \n",
       "4   26.0     1      1.11       6           11           0.0000      0.64   \n",
       "\n",
       "   temperatureHigh  apparentTemperatureHigh  uvIndex  precipIntensityMax  \\\n",
       "0            42.52                    40.53        0              0.0003   \n",
       "1            33.83                    32.85        0              0.0001   \n",
       "2            33.83                    32.85        0              0.0001   \n",
       "3            43.83                    38.38        0              0.1252   \n",
       "4            33.83                    32.85        0              0.0001   \n",
       "\n",
       "   temperatureMax  apparentTemperatureMax  \n",
       "0           42.52                   40.53  \n",
       "1           33.83                   32.85  \n",
       "2           33.83                   32.85  \n",
       "3           43.83                   38.38  \n",
       "4           33.83                   32.85  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/data_feature_engineering.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['price'])\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is a machine learning technique that builds models in a sequential manner. The gradient boosting algorithm combines predictions from multiple decision tress to general the best prediction. Each new model incrementally corrects errors made by previously trained models, using decision trees as the base. This approach makes gradient boosting particularly useful for analyzing complex datasets with nonlinear relationships and interaction effects. We chose gradient boosting as one of our models for its robustness and ability to handle various types of data efficiently, aiming to predict our target variable with high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest VS Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Random Forest & Gradient Boosting are very similar they differ in key ways. Gradient Boosting builds trees sequentially, with each tree correcting the errors made by the ensemble of previous trees. This iterative process focuses on improving the model's weaknesses. Random Forest builds multiple decision trees independently and combines their predictions through averaging or voting. Each tree is trained on a random subset of the data and features, enhancing diversity and reducing overfitting. Both methods leverage decision trees through ensemble learning, Gradient Boosting focuses on iterative improvement, while Random Forest emphasizes diversity and averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Mean Squared Error): 3.7984472773156606\n",
      "RMSE (Root Mean Squared Error): 1.9489605633043632\n",
      "RÂ² (Coefficient of Determination): 0.9475249975833804\n"
     ]
    }
   ],
   "source": [
    "#Model Initialization with GradientBoosting\n",
    "model_gb = GradientBoostingRegressor(n_estimators=500, learning_rate=.01, max_features=5, max_depth=5, random_state=42)\n",
    "# n_estimators=500: The number of trees in the forest. The algorithm will iteratively improve its predictions 500 times.\n",
    "# learning_rate=.01: This is the rate at which the model learns. A smaller learning rate requires more trees to model all the relationships but can lead to a more accurate model.\n",
    "# max_features=5: The maximum number of features to consider when looking for the best split which can help in making the model faster and reducing overfitting.\n",
    "# max_depth=5: The maximum depth of each tree. Limited depth helps control overfitting, making the model less complex.\n",
    "# random_state=42: A seed to  ensure reproducibility of the results.\n",
    "\n",
    "# Model Training\n",
    "model_gb.fit(X_train, y_train)\n",
    "# The model learns to predict the target variable y_train from the features X_train\n",
    "preds_test = model_gb.predict(X_test)\n",
    "# The model uses the learned relationships to predict the target variable for new data, X_test.\n",
    "\n",
    "#Evaluating the Model\n",
    "mse = mean_squared_error(y_test, preds_test)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, preds_test)\n",
    "# These metric are being used for all models to compare which model is best.\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"MSE (Mean Squared Error):\", mse)\n",
    "print(\"RMSE (Root Mean Squared Error):\", rmse)\n",
    "print(\"RÂ² (Coefficient of Determination):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing an initial iteration with Gradient Boosting using specific parameters, I've established a benchmark model that sets a baseline for performance metrics such as MSE, RMSE, and RÂ². This initial model serves as a starting point for further improvements. To enhance these performance metrics, I've used XGBoost and engaged in hyperparameter tuning with GridSearchCV. Extreme Gradient Boosting (XGBoost) is an efficient version of Gradient Boosting. It allows for similar results with fewer trees, making large data sets easier to process. This approach  explores various parameter configurations to identify the most effective model based on the negative mean squared error metric. Through this process of performance optimization, I aim to discover a set of hyperparameters that not only boosts the model's performance beyond the initial benchmark but also ensures its robustness and generalizability to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "Best parameters found:  {'alpha': 10, 'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200}\n",
      "Best score found:  1.859852786509415\n",
      "Best model:  XGBRegressor(alpha=10, base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=200, n_jobs=None,\n",
      "             num_parallel_tree=None, ...)\n",
      "Best model:  {'alpha': 10, 'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the XGBoost regressor with seed 42 to reproduce\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state = 42)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [5, 10],\n",
    "    'alpha': [5, 10],\n",
    "    'n_estimators': [100, 200, 500, 800]\n",
    "}\n",
    "\n",
    "# A dictionary named param_grid is created, specifying the parameters to be tuned and the range of values for each. This includes:\n",
    "    # - colsample_bytree: Fraction of features used per tree.\n",
    "    # - learning_rate: Step size shrinkage used to prevent overfitting.\n",
    "    # - max_depth: Maximum depth of the trees.\n",
    "    # - alpha: L1 regularization term on weights.\n",
    "    # - n_estimators: Number of trees in the ensemble.\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search_cv3 = GridSearchCV(estimator=xg_reg, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error', verbose=1, return_train_score=True)\n",
    "# GridSearchCV is used to explore different combinations of hyperparameters specified in param_grid to find the best performing model.\n",
    "    # - estimator: The  model you want to optimize.\n",
    "    # - param_grid: The hyperparameters to be tested\n",
    "    # - cv: Specifies the number of folds in a (Stratified) K-Fold cross-validation (3:2 test, 1 train - repeat 3 times)\n",
    "    # - n_jobs: This parameter tells the grid search to run in parallel\n",
    "    # - scoring: he metric used to evaluate the performance of the model for a given set of hyperparameters. Mean Squared Error (MSE), measures the average squared difference between the estimated values and the actual value. GridSearchCV aims to maximize the scoring metric; hence, MSE is negated since lower MSE values are better, and by negating it, the optimization problem becomes consistent.\n",
    "    # - verbose=1: This controls the verbosity (How much information is printed)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_cv3.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score with a cross-validation split of 3\n",
    "print(\"Best parameters found: \", grid_search_cv3.best_params_)\n",
    "print(\"Best score found: \", np.sqrt(-grid_search_cv3.best_score_))\n",
    "\n",
    "# Best estimator (model) with a cross-validation split of 3\n",
    "best_model3 = grid_search_cv3.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducting cross-validation (CV) with different fold numbers, within the XGBoost framework allows for a nuanced understanding of model performance across varying levels of data segmentation. This strategy helps in identifying the optimal balance between model training time and prediction accuracy. By comparing results across these different CV settings, one can better understand the trade-offs involved and select a CV strategy that aligns best with the project objectives\n",
    "\n",
    "CV is used ito assess how well a model will generalize to a data set. It involves splitting the data into training sets and a test set, then training the model on the training set and evaluating it on the test set. This process is repeated multiple times with each split taking a turn as the test set to produce a more accurate and less biased estimate of the model's performance. Cross-validation helps in identifying the model that performs best on unseen data, thereby reducing the likelihood of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best parameters found:  {'alpha': 10, 'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200}\n",
      "Best score found:  1.852872653020119\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup GridSearchCV with 5\n",
    "grid_search_cv5 = GridSearchCV(estimator=xg_reg, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=1, return_train_score=True)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_cv5.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score with a cross-validation split of 5\n",
    "print(\"Best parameters found: \", grid_search_cv5.best_params_)\n",
    "print(\"Best score found: \", np.sqrt(-grid_search_cv5.best_score_))\n",
    "\n",
    "# Best estimator (model) with a cross-validation split of 5\n",
    "best_model5 = grid_search_cv5.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In assessing the performance of XGBoost models using 3-fold and 5-fold cross-validation, it was observed that the model validated with 5 folds produced a marginally lower Root Mean Squared Error (RMSE) compared to the 3-fold cross-validation. This improvement in RMSE, although slight, indicates that a higher number of folds can offer a more refined estimate of the model's ability to generalize to unseen data, enhancing its predictive accuracy. Notably, both cross-validation strategies converged on the same optimial hyperparameters, emphasizing the robustness of the model's configuration. However, it's important to highlight that the increase in folds to CV=5 resulted in a substantially longer processing time. This trade-off between improved accuracy and increased computational demand emphasizes the need for a balanced approach, especially when considering the constraints of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Mean Squared Error): 3.3717752399377443\n",
      "RMSE (Root Mean Squared Error): 1.8362394288157915\n",
      "RÂ² (Coefficient of Determination): 0.9534194103678413\n"
     ]
    }
   ],
   "source": [
    "# Use the best estimator to make predictions\n",
    "predictions = best_model3.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"MSE (Mean Squared Error):\", mse)\n",
    "print(\"RMSE (Root Mean Squared Error):\", rmse)\n",
    "print(\"RÂ² (Coefficient of Determination):\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
